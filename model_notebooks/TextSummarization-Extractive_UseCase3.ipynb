{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            * Downloading libraries *                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk rouge-score\n",
    "!pip3 install torch==2.1.0 torchtext==0.16.0\n",
    "!pip3 install pandas\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install bert-extractive-summarizer\n",
    "!pip3 install numpy==1.22.4\n",
    "!pip3 install bert-extractive-summarizer transformers\n",
    "!pip3 install bert_score\n",
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip3 install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 cache purge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                Importing libraries                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import sentencepiece as spm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from summarizer import Summarizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, BertTokenizer, BertModel\n",
    "from bert_score import score as bert_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of summarization : \n",
    "1. Abstractive text summarization: The summary usually uses different words and phrases to concisely convey the same meaning as the original text.\n",
    "\n",
    "2. Extractive summarization: The summary contains the most important sentences from the original input text sentences without any paraphrasing or changes. The sentences deemed unnecessary are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models tried for Use Case 3 : \n",
    "\n",
    "1. BART (Bidirectional and Auto-Regressive Transformers) Model \n",
    "2. T5 (Text-to-Text Transfer Transformer) Model \n",
    "3. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "5. PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization)\n",
    "\n",
    "Examples of models for Extractive Text Summarization include : \n",
    "\n",
    "1. BERT : bert-base-uncased\n",
    "2. distilbert-base-uncased\n",
    "3. Sentence-BERT (extractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            Extractive BERT based pre-trained model                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            METRICS EVALUATION FOR TEXT SUMMARIZATION                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from csv\n",
    "def load_data(file_name):\n",
    "    df=pd.read_csv(file_name)\n",
    "    return df;\n",
    "\n",
    "def print_table(df):\n",
    "    # Determine the max length for each column\n",
    "    col_widths = {col: max(df[col].apply(lambda x: len(str(x)))) for col in df.columns}\n",
    "    \n",
    "    # Print the table header with padded columns\n",
    "    header = \" | \".join([col.ljust(col_widths[col]) for col in df.columns])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))  # Add a separator line\n",
    "    \n",
    "    # Print each row with padded columns\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \" | \".join([str(value).ljust(col_widths[col]) for col, value in row.items()])\n",
    "        print(row_str)\n",
    "    \n",
    "# Load the dataset from CSV\n",
    "df = load_data('test.csv')\n",
    "\n",
    "\n",
    "# Summarization and Evaluation function\n",
    "def summarize_and_evaluate(text, expected_summary,predicted_summary):\n",
    "\n",
    "    \n",
    "    # Ensure neither summary is None or empty\n",
    "    if not predicted_summary or not expected_summary:\n",
    "        print(\"One of the summaries is empty!\")\n",
    "        return 0  # or some default value\n",
    "\n",
    "    # Evaluate model performance\n",
    "    P, R, F1 = bert_score([predicted_summary], [expected_summary], lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    bert_score_value = F1.mean().item()\n",
    "    #factual_consistency_score = calculate_factual_consistency(predicted_summary, expected_summary)\n",
    "    return bert_score_value\n",
    "\n",
    "\n",
    "def main():\n",
    "        \n",
    "        results=[]\n",
    "      \n",
    "        # Iterate over the rows of the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                expected_summary = row['summary']\n",
    "\n",
    "                # Step 1: Initialize the Summarizer with a BERT-based model, which is 'bert-base-uncased'        \n",
    "                distilbert_model = Summarizer('distilbert-base-uncased')\n",
    "                \n",
    "                # Step 2: Perform Extractive Summarization\n",
    "                predicted_summary = distilbert_model(text, ratio=0.2)\n",
    "                #bert_score_val, factcc_val = summarize_and_evaluate(text, expected_summary,predicted_summary)\n",
    "                #bert_score_val,factcc_val = summarize_and_evaluate(text, expected_summary,predicted_summary)\n",
    "                bert_score_val = summarize_and_evaluate(text, expected_summary,predicted_summary)\n",
    "\n",
    "\n",
    "                '''\n",
    "                results.append({'S.No':index+1,'Extractive Summary': predicted_summary,'Original Text': text,\n",
    "                                'BERTScore': round(bert_score_val, 2), 'FactCC': round(factcc_val, 2), 'SummaC': round(summac_val, 2)\n",
    "                                })\n",
    "                '''   \n",
    "                results.append({'S.No':index+1,'Extractive Summary': predicted_summary,\n",
    "                                'BERTScore': round(bert_score_val, 2),\n",
    "                                'Original Text': text\n",
    "                                })\n",
    "                \n",
    "\n",
    "        # Convert the results list into a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Print the results in a padded tabular format\n",
    "        print_table(results_df)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            MODEL CREATED FOR EXTRACTIVE SUMMARIZATION                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from bert_score import score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shilpasingh/Desktop/Media-Insight/env39/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:12<00:00, 12.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15.5183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.8229\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:12<00:00, 12.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.9617\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:12<00:00, 12.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.3155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it]\n",
      "/Users/shilpasingh/Desktop/Media-Insight/env39/lib/python3.9/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.6287\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from CSV\n",
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['text'].tolist(), df['summary'].tolist()\n",
    "\n",
    "# Define the Dataset class\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        summary = self.summaries[idx]\n",
    "\n",
    "        # Tokenize text and summary\n",
    "        text_encoding = self.tokenizer(\n",
    "            text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        summary_encoding = self.tokenizer(\n",
    "            summary, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": summary_encoding[\"input_ids\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Custom collate function for dynamic padding\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=1)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Train function\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Main training loop\n",
    "def main():\n",
    "    # Load dataset\n",
    "    train_texts, train_summaries = load_dataset(\"test.csv\")\n",
    "    val_texts, val_summaries = load_dataset(\"test.csv\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SummarizationDataset(train_texts, train_summaries, tokenizer)\n",
    "    val_dataset = SummarizationDataset(val_texts, val_summaries, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Training\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss = evaluate_epoch(model, val_dataloader, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(\"summarization_model\")\n",
    "    tokenizer.save_pretrained(\"summarization_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
