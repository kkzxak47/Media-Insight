{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            * Downloading libraries *                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk rouge-score\n",
    "!pip3 install torch==2.1.0 torchtext==0.16.0  # Example, use versions compatible with each other\n",
    "!pip3 install pandas\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install bert-extractive-summarizer\n",
    "!pip3 install numpy==1.22.4\n",
    "!pip3 install bert-extractive-summarizer transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 cache purge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                Importing libraries                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import sentencepiece as spm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from summarizer import Summarizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of summarization : \n",
    "1. Abstractive text summarization: The summary usually uses different words and phrases to concisely convey the same meaning as the original text.\n",
    "\n",
    "2. Extractive summarization: The summary contains the most important sentences from the original input text sentences without any paraphrasing or changes. The sentences deemed unnecessary are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models tried for Use Case 3 : \n",
    "\n",
    "1. BART (Bidirectional and Auto-Regressive Transformers) Model \n",
    "2. T5 (Text-to-Text Transfer Transformer) Model \n",
    "3. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "5. PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization)\n",
    "\n",
    "Examples of models for Extractive Text Summarization include : \n",
    "\n",
    "1. BERT : bert-base-uncased\n",
    "2. distilbert-base-uncased\n",
    "3. Sentence-BERT (extractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            Extractive BERT based pre-trained model                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.No | Extractive Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Original Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1 | Ever noticed how plane seats appear to be getting smaller and smaller? More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. | Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee. 'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n",
      "2 | A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India. Brave fool: Fortunately, Mr Kumar  fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it. ' Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.'                                      | A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at the Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would 'kill them'. Mr Kumar explained afterwards that he was drunk and 'thought I'd stand a good chance' against the predators. Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!' Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security. The intoxicated teenager ran towards the lions, shouting: 'Today I kill a lion or a lion kills me!' A zoo spokesman said: 'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it. 'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out. 'We then handed him over to the police.' Brave fool: Fortunately, Mr Kumar  fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it. 'I was drunk and thought I'd stand a good chance.' A police spokesman said: 'He has been cautioned and will be sent for psychiatric evaluation. 'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.' Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "3 | Dougie Freedman is on the verge of agreeing a new two-year deal to remain at Nottingham Forest. Freedman has impressed at the City Ground since replacing Stuart Pearce in February .                                                                                                                                                                                                                                                                                                                                       | Dougie Freedman is on the verge of agreeing a new two-year deal to remain at Nottingham Forest. Freedman has stabilised Forest since he replaced cult hero Stuart Pearce and the club's owners are pleased with the job he has done at the City Ground. Dougie Freedman is set to sign a new deal at Nottingham Forest . Freedman has impressed at the City Ground since replacing Stuart Pearce in February . They made an audacious attempt on the play-off places when Freedman replaced Pearce but have tailed off in recent weeks. That has not prevented Forest's ownership making moves to secure Freedman on a contract for the next two seasons.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "4 | Liverpool target Neto is also wanted by PSG and clubs in Spain as Brendan Rodgers faces stiff competition to land the Fiorentina goalkeeper, according to the Brazilian's agent Stefano Castagna. A January move for Neto never materialised but the former Atletico Paranaense keeper looks certain to leave the Florence-based club in the summer. Neto is wanted by a number of top European clubs including Liverpool and PSG, according to his agent .                                                                 | Liverpool target Neto is also wanted by PSG and clubs in Spain as Brendan Rodgers faces stiff competition to land the Fiorentina goalkeeper, according to the Brazilian's agent Stefano Castagna. The Reds were linked with a move for the 25-year-old, whose contract expires in June, earlier in the season when Simon Mignolet was dropped from the side. A January move for Neto never materialised but the former Atletico Paranaense keeper looks certain to leave the Florence-based club in the summer. Neto rushes from his goal as Juan Iturbe bears down on him during Fiorentina's clash with Roma in March . Neto is wanted by a number of top European clubs including Liverpool and PSG, according to his agent . It had been reported that Neto had a verbal agreement to join Serie A champions Juventus at the end of the season but his agent has revealed no decision about his future has been made yet. And Castagna claims Neto will have his pick of top European clubs when the transfer window re-opens in the summer, including Brendan Rodgers' side. 'There are many European clubs interested in Neto, such as for example Liverpool and Paris Saint-Germain,' Stefano Castagna is quoted as saying by Gazzetta TV. Firoentina goalkeeper Neto saves at the feet of Tottenham midfielder Nacer Chadli in the Europa League . 'In Spain too there are clubs at the very top level who are tracking him. Real Madrid? We'll see. 'We have not made a definitive decision, but in any case he will not accept another loan move elsewhere.' Neto, who represented Brazil at the London 2012 Olympics but has not featured for the senior side, was warned against joining a club as a No 2 by national coach Dunga. Neto joined Fiorentina from Atletico Paranaense in 2011 and established himself as No1 in the last two seasons.                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset from csv\n",
    "def load_data(file_name):\n",
    "    df=pd.read_csv(file_name)\n",
    "    return df;\n",
    "\n",
    "def print_table(df):\n",
    "    # Determine the max length for each column\n",
    "    col_widths = {col: max(df[col].apply(lambda x: len(str(x)))) for col in df.columns}\n",
    "    \n",
    "    # Print the table header with padded columns\n",
    "    header = \" | \".join([col.ljust(col_widths[col]) for col in df.columns])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))  # Add a separator line\n",
    "    \n",
    "    # Print each row with padded columns\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \" | \".join([str(value).ljust(col_widths[col]) for col, value in row.items()])\n",
    "        print(row_str)\n",
    "    \n",
    "# Load the dataset from CSV\n",
    "df = load_data('test.csv')\n",
    "\n",
    "def main():\n",
    "        \n",
    "        results=[]\n",
    "      \n",
    "        # Iterate over the rows of the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                expected_summary = row['summary']\n",
    "\n",
    "                # Step 1: Initialize the Summarizer with a BERT-based model, which is 'bert-base-uncased'        \n",
    "                distilbert_model = Summarizer('distilbert-base-uncased')\n",
    "                \n",
    "                # Step 2: Perform Extractive Summarization\n",
    "                predicted_summary = distilbert_model(text, ratio=0.2)\n",
    "\n",
    "                # Step 3: Print the Summary\n",
    "                #print(\"\\nOriginal Text:\\n\", text)\n",
    "                #print(\"\\nExtractive Summary:\\n\", predicted_summary)\n",
    "\n",
    "                results.append({'S.No':index+1,'Extractive Summary': predicted_summary,'Original Text': text})\n",
    "\n",
    "        # Convert the results list into a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Print the results in a padded tabular format\n",
    "        print_table(results_df)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            MODEL CREATED FOR EXTRACTIVE SUMMARIZATION                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Sample Document (You would replace this with multiple documents in a real setting)\n",
    "text = \"\"\"\n",
    "How does the BART summarization model compare to the other summarization models out there? Research groups still compare these models using the old recall-oriented understudy for gisting evaluation (ROUGE) metrics. But ROUGE looks for common words and n-grams between the generated and reference summaries — the more there are, the higher the score. Since abstractive models paraphrase the text, they may not score well, and high scores may not result in good summaries under real-world conditions.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Convert each sentence into an embedding vector (using random embeddings for simplicity)\n",
    "# In a real scenario, you'd use pre-trained embeddings like GloVe or BERT\n",
    "embedding_dim = 50\n",
    "sentence_embeddings = [np.random.rand(embedding_dim) for _ in sentences]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "# Step 3: Create labeled data (for demonstration, let's label the first 2 sentences as \"important\")\n",
    "labels = [1 if i < 2 else 0 for i in range(len(sentences))]  # 1: Important, 0: Not Important\n",
    "labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)  # Ensure the shape is [batch_size, 1]\n",
    "\n",
    "# Step 4: Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Step 5: Define a simple neural network model for scoring sentences\n",
    "class SentenceScoringModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SentenceScoringModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is 3D tensor of shape (batch_size, seq_len=1, input_size)\n",
    "        #x = x.unsqueeze(1)  # Reshape to (batch_size, seq_len=1, input_size)\n",
    "        \n",
    "        # Forward pass through LSTM\n",
    "        _, (hidden, _) = self.lstm(x)  # LSTM output\n",
    "        output = self.fc(hidden[-1])  # Feed to fully connected layer\n",
    "        return self.sigmoid(output)  # Sigmoid output between 0 and 1\n",
    "\n",
    "# Instantiate model, define loss function and optimizer\n",
    "model = SentenceScoringModel(input_dim=embedding_dim)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 6: Train the model\n",
    "num_epochs = 100\n",
    "batch_size = 2  # Define batch size\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: Ensure correct dimensions\n",
    "    num_batches = len(X_train) // batch_size  # Calculate number of batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Select a batch of input and target data\n",
    "        X_batch = X_train[start_idx:end_idx]  # Shape: [batch_size, input_size]\n",
    "        y_batch = y_train[start_idx:end_idx]  # Shape: [batch_size, 1]\n",
    "\n",
    "        # Ensure input tensor is in the correct shape (3D tensor)\n",
    "        X_batch = X_batch.unsqueeze(1)  # Reshape to (batch_size, seq_len=1, input_size)\n",
    "        \n",
    "        \n",
    "        # Forward pass through the model\n",
    "        predictions = model(X_batch).view(batch_size, 1)  # Predictions should have shape [batch_size, 1]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 7: Evaluate the model on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test).view(-1, 1)  # Ensure test predictions are [batch_size, 1]\n",
    "    test_loss = criterion(test_predictions, y_test)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Step 8: Generate Summary\n",
    "# Run the model on all sentences in the document to score them\n",
    "sentence_scores = []\n",
    "with torch.no_grad():\n",
    "    for sentence_embedding in sentence_embeddings:\n",
    "        sentence_embedding_tensor = torch.tensor(sentence_embedding, dtype=torch.float32).view(1, -1)  # Reshape for batch size\n",
    "        sentence_embedding_tensor = sentence_embedding_tensor.unsqueeze(1)  # Shape: [1, 1, input_size]\n",
    "        \n",
    "        # Debugging output for tensor shape\n",
    "        print(f'Sentence input shape: {sentence_embedding_tensor.shape}')\n",
    "        \n",
    "        score = model(sentence_embedding_tensor)\n",
    "        sentence_scores.append(score.item())\n",
    "\n",
    "# Select the top N sentences with the highest scores for the summary\n",
    "N = 2  # Number of sentences for the summary\n",
    "top_sentence_indices = np.argsort(sentence_scores)[-N:][::-1]\n",
    "summary = ' '.join([sentences[i] for i in top_sentence_indices])\n",
    "\n",
    "print(\"\\n text:\\n\",text)\n",
    "print(\"\\nExtractive Summary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
