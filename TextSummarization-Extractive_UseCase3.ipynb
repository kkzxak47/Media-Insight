{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            * Downloading libraries *                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk rouge-score\n",
    "!pip3 install torch==2.1.0 torchtext==0.16.0  # Example, use versions compatible with each other\n",
    "!pip3 install pandas\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install bert-extractive-summarizer\n",
    "#!pip3 install numpy==1.22.4\n",
    "!pip3 install numpy==1.23\n",
    "!pip3 install bert-extractive-summarizer transformers\n",
    "!pip3 install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy==1.23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 cache purge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                Importing libraries                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import sentencepiece as spm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from summarizer import Summarizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, BertTokenizer, BertModel\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of summarization : \n",
    "1. Abstractive text summarization: The summary usually uses different words and phrases to concisely convey the same meaning as the original text.\n",
    "\n",
    "2. Extractive summarization: The summary contains the most important sentences from the original input text sentences without any paraphrasing or changes. The sentences deemed unnecessary are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models tried for Use Case 3 : \n",
    "\n",
    "1. BART (Bidirectional and Auto-Regressive Transformers) Model \n",
    "2. T5 (Text-to-Text Transfer Transformer) Model \n",
    "3. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "5. PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization)\n",
    "\n",
    "Examples of models for Extractive Text Summarization include : \n",
    "\n",
    "1. BERT : bert-base-uncased\n",
    "2. distilbert-base-uncased\n",
    "3. Sentence-BERT (extractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            Extractive BERT based pre-trained model                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from csv\n",
    "def load_data(file_name):\n",
    "    df=pd.read_csv(file_name)\n",
    "    return df;\n",
    "\n",
    "def print_table(df):\n",
    "    # Determine the max length for each column\n",
    "    col_widths = {col: max(df[col].apply(lambda x: len(str(x)))) for col in df.columns}\n",
    "    \n",
    "    # Print the table header with padded columns\n",
    "    header = \" | \".join([col.ljust(col_widths[col]) for col in df.columns])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))  # Add a separator line\n",
    "    \n",
    "    # Print each row with padded columns\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \" | \".join([str(value).ljust(col_widths[col]) for col, value in row.items()])\n",
    "        print(row_str)\n",
    "    \n",
    "# Load the dataset from CSV\n",
    "df = load_data('test.csv')\n",
    "\n",
    "def main():\n",
    "        \n",
    "        results=[]\n",
    "      \n",
    "        # Iterate over the rows of the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                expected_summary = row['summary']\n",
    "\n",
    "                # Step 1: Initialize the Summarizer with a BERT-based model, which is 'bert-base-uncased'        \n",
    "                distilbert_model = Summarizer('distilbert-base-uncased')\n",
    "                \n",
    "                # Step 2: Perform Extractive Summarization\n",
    "                predicted_summary = distilbert_model(text, ratio=0.2)\n",
    "                #bert_score_val, factcc_val = summarize_and_evaluate(text, expected_summary,predicted_summary)\n",
    "                bert_score_val,factcc_val = summarize_and_evaluate(text, expected_summary,predicted_summary)\n",
    "\n",
    "\n",
    "                results.append({'S.No':index+1,'Extractive Summary': predicted_summary,'Original Text': text,\n",
    "                                'BERTScore': round(bert_score_val, 2), 'FactCC': round(factcc_val, 2), 'SummaC': round(summac_val, 2)\n",
    "                                })\n",
    "\n",
    "        # Convert the results list into a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Print the results in a padded tabular format\n",
    "        print_table(results_df)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            METRICS EVALUATION FOR TEXT SUMMARIZATION                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as bert_score\n",
    "# Calculate factual consistency using entailment (alternative to FactCC)\n",
    "def calculate_factual_consistency(predicted_summary, reference_summary):\n",
    "    result = entailment_model(f\"Summary: {predicted_summary} Text: {reference_summary}\")\n",
    "    score = 1 if result[0]['label'] == 'ENTAILMENT' else 0\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Summarization and Evaluation function\n",
    "def summarize_and_evaluate(text, expected_summary,predicted_summary):\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    P, R, F1 = bert_score([predicted_summary], [expected_summary], lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_score_value = F1.mean().item()\n",
    "    factual_consistency_score = calculate_factual_consistency(predicted_summary, expected_summary)\n",
    "    return bert_score_value, factual_consistency_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            MODEL CREATED FOR EXTRACTIVE SUMMARIZATION                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Sample Document (You would replace this with multiple documents in a real setting)\n",
    "text = \"\"\"\n",
    "How does the BART summarization model compare to the other summarization models out there? Research groups still compare these models using the old recall-oriented understudy for gisting evaluation (ROUGE) metrics. But ROUGE looks for common words and n-grams between the generated and reference summaries â€” the more there are, the higher the score. Since abstractive models paraphrase the text, they may not score well, and high scores may not result in good summaries under real-world conditions.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Step 2: Convert each sentence into an embedding vector (using random embeddings for simplicity)\n",
    "# In a real scenario, you'd use pre-trained embeddings like GloVe or BERT\n",
    "embedding_dim = 50\n",
    "sentence_embeddings = [np.random.rand(embedding_dim) for _ in sentences]\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "# Step 3: Create labeled data (for demonstration, let's label the first 2 sentences as \"important\")\n",
    "labels = [1 if i < 2 else 0 for i in range(len(sentences))]  # 1: Important, 0: Not Important\n",
    "labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)  # Ensure the shape is [batch_size, 1]\n",
    "\n",
    "# Step 4: Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Step 5: Define a simple neural network model for scoring sentences\n",
    "class SentenceScoringModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SentenceScoringModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size=32, batch_first=True)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is 3D tensor of shape (batch_size, seq_len=1, input_size)\n",
    "        #x = x.unsqueeze(1)  # Reshape to (batch_size, seq_len=1, input_size)\n",
    "        \n",
    "        # Forward pass through LSTM\n",
    "        _, (hidden, _) = self.lstm(x)  # LSTM output\n",
    "        output = self.fc(hidden[-1])  # Feed to fully connected layer\n",
    "        return self.sigmoid(output)  # Sigmoid output between 0 and 1\n",
    "\n",
    "# Instantiate model, define loss function and optimizer\n",
    "model = SentenceScoringModel(input_dim=embedding_dim)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 6: Train the model\n",
    "num_epochs = 100\n",
    "batch_size = 2  # Define batch size\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: Ensure correct dimensions\n",
    "    num_batches = len(X_train) // batch_size  # Calculate number of batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Select a batch of input and target data\n",
    "        X_batch = X_train[start_idx:end_idx]  # Shape: [batch_size, input_size]\n",
    "        y_batch = y_train[start_idx:end_idx]  # Shape: [batch_size, 1]\n",
    "\n",
    "        # Ensure input tensor is in the correct shape (3D tensor)\n",
    "        X_batch = X_batch.unsqueeze(1)  # Reshape to (batch_size, seq_len=1, input_size)\n",
    "        \n",
    "        \n",
    "        # Forward pass through the model\n",
    "        predictions = model(X_batch).view(batch_size, 1)  # Predictions should have shape [batch_size, 1]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 7: Evaluate the model on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test).view(-1, 1)  # Ensure test predictions are [batch_size, 1]\n",
    "    test_loss = criterion(test_predictions, y_test)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Step 8: Generate Summary\n",
    "# Run the model on all sentences in the document to score them\n",
    "sentence_scores = []\n",
    "with torch.no_grad():\n",
    "    for sentence_embedding in sentence_embeddings:\n",
    "        sentence_embedding_tensor = torch.tensor(sentence_embedding, dtype=torch.float32).view(1, -1)  # Reshape for batch size\n",
    "        sentence_embedding_tensor = sentence_embedding_tensor.unsqueeze(1)  # Shape: [1, 1, input_size]\n",
    "        \n",
    "        # Debugging output for tensor shape\n",
    "        print(f'Sentence input shape: {sentence_embedding_tensor.shape}')\n",
    "        \n",
    "        score = model(sentence_embedding_tensor)\n",
    "        sentence_scores.append(score.item())\n",
    "\n",
    "# Select the top N sentences with the highest scores for the summary\n",
    "N = 2  # Number of sentences for the summary\n",
    "top_sentence_indices = np.argsort(sentence_scores)[-N:][::-1]\n",
    "summary = ' '.join([sentences[i] for i in top_sentence_indices])\n",
    "\n",
    "print(\"\\n text:\\n\",text)\n",
    "print(\"\\nExtractive Summary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
